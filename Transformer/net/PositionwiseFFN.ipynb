{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFFN(nn.Module):\n",
    "    \"\"\"PositionWiseFFN from Attention is All You Need, dl2.ai, and Cohen et. al (2022).\n",
    "        Two-layer MLP applied to each index of the input sequence.\n",
    "        Parameters\n",
    "        ----------        \n",
    "        d_model: dimension of latent space in the model.\n",
    "        d_ffn_hidden: defaults to 2048. Dimension of hidden layer between MLPs.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, d_ffn_hidden=2048):\n",
    "        \"\"\"Default hidden dimension is 2048\"\"\"\n",
    "        super().__init__()\n",
    "        self._linear1 = nn.Linear(d_model, d_ffn_hidden)\n",
    "        self._relu = nn.ReLU()\n",
    "        self._linear2 = nn.Linear(d_ffn_hidden, d_model)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Pass x through the PositionwiseFFN block  Input and output have a shape (d_model, d_ffn_hidden).\n",
    "        Parameters\n",
    "        ----------\n",
    "        x: \n",
    "            Input tensor of shape (batch_size, input_len, d_model)\n",
    "        Returns\n",
    "        ----------\n",
    "        x:\n",
    "            Output tensor of shape (batch_size, input_len, d_model)\n",
    "        \"\"\"\n",
    "        x = self._linear1(x)\n",
    "        x = self._relu(x)\n",
    "        return self._linear2(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmodel=64\n",
    "inlen = 60\n",
    "ffn = PositionwiseFFN(dmodel)\n",
    "x = torch.rand(32, 10, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = ffn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2851,  0.1430,  0.1040,  ...,  0.2884, -0.4036, -0.0844],\n",
       "         [ 0.2177,  0.1151,  0.0471,  ...,  0.0714, -0.2473, -0.0229],\n",
       "         [ 0.2511,  0.0610,  0.0490,  ...,  0.2193, -0.2525, -0.0742],\n",
       "         ...,\n",
       "         [ 0.2412,  0.0759, -0.0226,  ...,  0.1360, -0.3927, -0.0599],\n",
       "         [ 0.3453,  0.0892,  0.0219,  ...,  0.0996, -0.3204, -0.0642],\n",
       "         [ 0.2717,  0.0913,  0.0785,  ...,  0.1144, -0.2010,  0.1214]],\n",
       "\n",
       "        [[ 0.2064, -0.0324, -0.0077,  ...,  0.1458, -0.2877,  0.0661],\n",
       "         [ 0.3164,  0.0321,  0.0240,  ...,  0.0603, -0.2975, -0.0182],\n",
       "         [ 0.1927,  0.0828,  0.0341,  ...,  0.1114, -0.3281, -0.0177],\n",
       "         ...,\n",
       "         [ 0.3543,  0.2046,  0.1208,  ...,  0.0940, -0.2701, -0.0550],\n",
       "         [ 0.2006, -0.0670,  0.1534,  ...,  0.0621, -0.3204, -0.1335],\n",
       "         [ 0.2756,  0.1128,  0.0745,  ...,  0.1781, -0.3932, -0.0874]],\n",
       "\n",
       "        [[ 0.2642,  0.0309,  0.0981,  ...,  0.1251, -0.3062, -0.0804],\n",
       "         [ 0.2680,  0.0025,  0.0126,  ...,  0.1424, -0.2652, -0.1113],\n",
       "         [ 0.2953,  0.1013,  0.0304,  ...,  0.1106, -0.3958,  0.0404],\n",
       "         ...,\n",
       "         [ 0.1990,  0.1283,  0.0962,  ...,  0.1299, -0.2938, -0.0534],\n",
       "         [ 0.2099,  0.0507,  0.0549,  ...,  0.1042, -0.2966,  0.0430],\n",
       "         [ 0.1972,  0.0865,  0.0292,  ...,  0.1745, -0.3089, -0.0947]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.3653,  0.1374,  0.0896,  ...,  0.1982, -0.2630, -0.0415],\n",
       "         [ 0.1568,  0.0779,  0.0547,  ...,  0.1535, -0.3827, -0.0212],\n",
       "         [ 0.2599,  0.0172, -0.0147,  ...,  0.1803, -0.2713, -0.1491],\n",
       "         ...,\n",
       "         [ 0.2993,  0.1291,  0.1032,  ...,  0.1502, -0.2364, -0.0877],\n",
       "         [ 0.2481,  0.0040,  0.0570,  ...,  0.0951, -0.2231, -0.0538],\n",
       "         [ 0.2126,  0.1777,  0.0655,  ...,  0.1717, -0.0899,  0.0551]],\n",
       "\n",
       "        [[ 0.3363,  0.0220,  0.1402,  ...,  0.1388, -0.4104, -0.0011],\n",
       "         [ 0.2767,  0.1114,  0.0462,  ...,  0.1862, -0.3925, -0.1105],\n",
       "         [ 0.3656,  0.1492,  0.0945,  ...,  0.0520, -0.2295, -0.0808],\n",
       "         ...,\n",
       "         [ 0.2416,  0.0890,  0.0012,  ...,  0.1617, -0.1445, -0.0806],\n",
       "         [ 0.1022, -0.0362,  0.1019,  ...,  0.1435, -0.4169,  0.0594],\n",
       "         [ 0.2602,  0.0566,  0.1321,  ...,  0.0270, -0.3761, -0.0325]],\n",
       "\n",
       "        [[ 0.3784,  0.1386,  0.0416,  ...,  0.0210, -0.3358,  0.0864],\n",
       "         [ 0.2734,  0.0107,  0.0133,  ...,  0.1125, -0.3908, -0.0686],\n",
       "         [ 0.2226,  0.1099,  0.0325,  ...,  0.0900, -0.3177,  0.0052],\n",
       "         ...,\n",
       "         [ 0.3366,  0.0805, -0.0106,  ...,  0.1679, -0.2615, -0.0164],\n",
       "         [ 0.3124,  0.1510,  0.0796,  ...,  0.1706, -0.2826, -0.0463],\n",
       "         [ 0.2445,  0.0314,  0.1384,  ...,  0.0780, -0.2308, -0.1704]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d65bdc5d2ce5f47a5dacc71f251d002481db9b35ba5522b1e89f66f66f443b2e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
