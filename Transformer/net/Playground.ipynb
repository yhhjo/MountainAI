{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from PositionwiseFFN import PositionwiseFFN\n",
    "from DotProductAttention import DotProductAttention\n",
    "from MultiHeadAttention import MultiHeadAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test PFFN\n",
    "dmodel=64\n",
    "inlen = 60\n",
    "ffn = PositionwiseFFN(dmodel)\n",
    "x = torch.rand(32, 10, 64)\n",
    "assert ffn(x).shape == x.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test dot queries = torch.normal(0, 1, (2, 1, 2))\n",
    "### Test code from d2l.ai\n",
    "queries = torch.normal(0, 1, (2, 10, 2))\n",
    "keys = torch.normal(0, 1, (2, 10, 2))\n",
    "values = torch.normal(0, 1, (2, 10, 4))\n",
    "valid_lens = torch.tensor([2, 6])\n",
    "\n",
    "attention = DotProductAttention(dropout=0.5)\n",
    "attention.eval()\n",
    "assert attention(queries, keys, values).shape == (2, 10, 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test MHA\n",
    "d_model, num_heads = 100, 5\n",
    "attention = MultiHeadAttention(d_model, d_model, d_model, num_heads)\n",
    "batch_size = 2\n",
    "\n",
    "X = torch.ones((batch_size, inlen, d_model))\n",
    "\n",
    "assert attention(X, X, X, \"None\").shape == (batch_size, inlen, d_model)\n",
    "assert attention(X, X, X, \"future\").shape == (batch_size, inlen, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I = tensor([[[2.8006,   -inf,   -inf,   -inf,   -inf],\n",
      "         [1.2165, 2.0385,   -inf,   -inf,   -inf],\n",
      "         [2.1119, 1.6105, 2.7295,   -inf,   -inf],\n",
      "         [2.4833, 2.5296, 2.7958, 4.0144,   -inf],\n",
      "         [2.4044, 1.8091, 1.8190, 2.8045, 3.2882]],\n",
      "\n",
      "        [[1.0388,   -inf,   -inf,   -inf,   -inf],\n",
      "         [0.7718, 1.0677,   -inf,   -inf,   -inf],\n",
      "         [1.4157, 1.3924, 2.8830,   -inf,   -inf],\n",
      "         [0.9099, 0.6977, 1.2599, 1.2861,   -inf],\n",
      "         [1.3134, 1.2860, 1.9563, 0.8996, 2.5729]]])\n",
      "Attention = tensor([[[0.9416, 0.8047, 0.8595, 0.8868, 0.8382, 0.5065, 0.6264, 0.4394],\n",
      "         [0.5185, 0.8382, 0.7014, 0.8138, 0.5614, 0.3301, 0.3265, 0.6241],\n",
      "         [0.5208, 0.4814, 0.5057, 0.6066, 0.5135, 0.4826, 0.3880, 0.5943],\n",
      "         [0.4516, 0.5290, 0.6525, 0.6475, 0.5607, 0.3722, 0.5285, 0.3947],\n",
      "         [0.6134, 0.6087, 0.6090, 0.6030, 0.4233, 0.4834, 0.5184, 0.4084]],\n",
      "\n",
      "        [[0.5517, 0.2241, 0.8043, 0.6366, 0.1877, 0.9753, 0.4362, 0.0721],\n",
      "         [0.3938, 0.1382, 0.4670, 0.6563, 0.2507, 0.9479, 0.2821, 0.1187],\n",
      "         [0.3839, 0.1822, 0.6815, 0.7037, 0.4087, 0.6298, 0.3587, 0.3664],\n",
      "         [0.3830, 0.2336, 0.5424, 0.7014, 0.3799, 0.7770, 0.3902, 0.4204],\n",
      "         [0.2694, 0.2494, 0.5329, 0.5970, 0.3945, 0.5291, 0.4508, 0.4772]]])\n"
     ]
    }
   ],
   "source": [
    "# Visualize shaping of attention and masks\n",
    "q = torch.rand((2,5,8))\n",
    "v = torch.rand((2,5,8))\n",
    "k = q\n",
    "\n",
    "K = q.shape[1]  # Sequence length\n",
    "\n",
    "dk = q.shape[1]**.5 \n",
    "I = (q@k.transpose(1,2))\n",
    "\n",
    "future_mask = torch.triu(torch.ones((K, K)), diagonal=1).bool()\n",
    "I = I.masked_fill(future_mask, float('-inf'))\n",
    "a = nn.functional.softmax(I/dk, dim=-1)@v \n",
    "print(f\"I = {I}\")\n",
    "print(f\"Attention = {a}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    \"\"\"Transformer Encoder Block from Attention is All You Need. \n",
    "    Layer 1: MHA\n",
    "    Layer 2: Layer normalization + residual connection\n",
    "    Layer 3: Positionwise FFN \n",
    "    Layer 4: Layer normalization + residual connection\n",
    "    \n",
    "    Parameters \n",
    "        ----------\n",
    "        d_model:\n",
    "            Dimension model's latent space\n",
    "        q:\n",
    "            Query tensor shape (batch_size, K, d_model)\n",
    "        k:\n",
    "            Key tensor shape (batch_size, K, d_model)\n",
    "        v:\n",
    "            Value tensor shape (batch_size, K, d_model)\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        Self attention:\n",
    "            Tensor shape (batch_size, K, d_model)\n",
    "\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d65bdc5d2ce5f47a5dacc71f251d002481db9b35ba5522b1e89f66f66f443b2e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
