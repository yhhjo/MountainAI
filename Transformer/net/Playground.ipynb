{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from PositionwiseFFN import PositionwiseFFN\n",
    "from DotProductAttention import DotProductAttention, MASKS\n",
    "from MultiHeadAttention import MultiHeadAttention\n",
    "from Encoder import EncoderBlock\n",
    "from Decoder import DecoderBlock\n",
    "from Transformer import Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test PFFN\n",
    "d_model=64\n",
    "ffn = PositionwiseFFN(d_model)\n",
    "x = torch.rand(32, 10, d_model)\n",
    "\n",
    "assert ffn(x).shape == x.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test dot queries = torch.normal(0, 1, (2, 1, 2))\n",
    "### Test code from d2l.ai\n",
    "queries = torch.normal(0, 1, (2, 10, 2))\n",
    "keys = torch.normal(0, 1, (2, 10, 2))\n",
    "values = torch.normal(0, 1, (2, 10, 4))\n",
    "valid_lens = torch.tensor([2, 6])\n",
    "\n",
    "attention = DotProductAttention(dropout=0.5)\n",
    "attention.eval()\n",
    "assert attention(queries, keys, values).shape == (2, 10, 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test MHA\n",
    "d_model, num_heads, inlen = 100, 5, 20\n",
    "attention = MultiHeadAttention(d_model, d_model, d_model, num_heads)\n",
    "batch_size = 2\n",
    "\n",
    "X = torch.ones((batch_size, inlen, d_model))\n",
    "\n",
    "assert attention(X, X, X, \"None\").shape == (batch_size, inlen, d_model)\n",
    "assert attention(X, X, X, \"future\").shape == (batch_size, inlen, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize shaping of attention and masks\n",
    "q = torch.rand((2,5,8))\n",
    "v = torch.rand((2,5,8))\n",
    "k = q\n",
    "\n",
    "K = q.shape[1]  # Sequence length\n",
    "\n",
    "dk = q.shape[1]**.5 \n",
    "I = (q@k.transpose(1,2))\n",
    "\n",
    "future_mask = torch.triu(torch.ones((K, K)), diagonal=1).bool()\n",
    "I = I.masked_fill(future_mask, float('-inf'))\n",
    "a = nn.functional.softmax(I/dk, dim=-1)@v \n",
    "#print(f\"I = {I}\")\n",
    "#print(f\"Attention = {a}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Encoder\n",
    "dmodel=64\n",
    "e = EncoderBlock(dmodel, dmodel, dmodel, h=8, dropout=0.2)\n",
    "x = torch.rand(32, 10, dmodel)\n",
    "assert e(x).shape == x.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Decoder\n",
    "dmodel=64\n",
    "q = v = 10\n",
    "e = DecoderBlock(dmodel, dmodel, dmodel, h=8, dropout=0.2)\n",
    "x = torch.rand(32, 10, dmodel)\n",
    "m = torch.rand(32, 10, dmodel)\n",
    "assert e(x, m).shape == x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the Decoder \n",
    "\n",
    "batch_size = 32\n",
    "d_model = 64  \n",
    "N_WORKERS = 0\n",
    "\n",
    "d_input = 10\n",
    "d_output = 7+3*5+5  \n",
    "q = 8\n",
    "v = 8\n",
    "h = 8\n",
    "Ne = Nd = 10\n",
    "\n",
    "optimizePrime = Transformer(d_input, d_output, d_model, q, v, h, Ne, Nd)\n",
    "x = torch.rand(batch_size, 1, d_input)\n",
    "assert optimizePrime(x).shape == (batch_size, 1, d_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_input(x)->np.array([]):\n",
    "    \"\"\"Takes input of shape H_thermal and concatenates the normalized vectors (age, AHe, Aft, ZHe, Zft)\n",
    "    Output is shape d_input. \n",
    "    \"\"\"\n",
    "    x = np.array([*x])[1:,:]\n",
    "    std = np.std(x, axis=1, keepdims=True)\n",
    "    std[std==0] = .00001\n",
    "    x = ((x - np.mean(x, axis=1, keepdims=True)) / std).T\n",
    "    return x\n",
    "\n",
    "\n",
    "def formattedLoader(d_input, d_output, dataset_path):\n",
    "    \"\"\"Returns data formatted specifically for transformer\"\"\"\n",
    "    dataset = np.load(dataset_path,allow_pickle=True)\n",
    "    formatted_data = []\n",
    "    for x, y in dataset:\n",
    "        x = process_input(x)\n",
    "        y = process_output(y, d_output)\n",
    "        formatted_data.append((x,y))\n",
    "    return formatted_data\n",
    "\n",
    "def rawLoader(dataset_path: str)->np.array([]):\n",
    "    return np.load(dataset_path,allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'Sample'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../../Dataset/CC10k.npy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/numpy/lib/npyio.py:440\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    438\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m.\u001b[39mopen_memmap(file, mode\u001b[38;5;241m=\u001b[39mmmap_mode)\n\u001b[1;32m    439\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 440\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_pickle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    441\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mpickle_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpickle_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    443\u001b[0m     \u001b[38;5;66;03m# Try a pickle\u001b[39;00m\n\u001b[1;32m    444\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_pickle:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/numpy/lib/format.py:748\u001b[0m, in \u001b[0;36mread_array\u001b[0;34m(fp, allow_pickle, pickle_kwargs)\u001b[0m\n\u001b[1;32m    746\u001b[0m     pickle_kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 748\u001b[0m     array \u001b[38;5;241m=\u001b[39m \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    749\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mUnicodeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    750\u001b[0m     \u001b[38;5;66;03m# Friendlier error message\u001b[39;00m\n\u001b[1;32m    751\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mUnicodeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnpickling a python object failed: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    752\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou may need to pass the encoding= option \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    753\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto numpy.load\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (err,)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'Sample'"
     ]
    }
   ],
   "source": [
    "dataset = np.load('../../Dataset/CC10k.npy',allow_pickle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d65bdc5d2ce5f47a5dacc71f251d002481db9b35ba5522b1e89f66f66f443b2e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
