{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from PositionwiseFFN import PositionwiseFFN\n",
    "from DotProductAttention import DotProductAttention\n",
    "from MultiHeadAttention import MultiHeadAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test PFFN\n",
    "dmodel=64\n",
    "inlen = 60\n",
    "ffn = PositionwiseFFN(dmodel)\n",
    "x = torch.rand(32, 10, 64)\n",
    "assert ffn(x).shape == x.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test dot queries = torch.normal(0, 1, (2, 1, 2))\n",
    "### Test code from d2l.ai\n",
    "queries = torch.normal(0, 1, (2, 10, 2))\n",
    "keys = torch.normal(0, 1, (2, 10, 2))\n",
    "values = torch.normal(0, 1, (2, 10, 4))\n",
    "valid_lens = torch.tensor([2, 6])\n",
    "\n",
    "attention = DotProductAttention(dropout=0.5)\n",
    "attention.eval()\n",
    "assert attention(queries, keys, values).shape == (2, 10, 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test MHA\n",
    "d_model, num_heads = 100, 5\n",
    "attention = MultiHeadAttention(d_model, num_heads)\n",
    "batch_size = 2\n",
    "\n",
    "X = torch.ones((batch_size, inlen, d_model))\n",
    "\n",
    "assert attention(X, X, X, \"None\").shape == (batch_size, inlen, d_model)\n",
    "assert attention(X, X, X, \"future\").shape == (batch_size, inlen, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I = tensor([[[2.5095,   -inf,   -inf,   -inf,   -inf],\n",
      "         [1.7438, 2.0668,   -inf,   -inf,   -inf],\n",
      "         [2.5143, 1.6206, 3.5345,   -inf,   -inf],\n",
      "         [1.4594, 1.4089, 1.6951, 1.7672,   -inf],\n",
      "         [2.4159, 1.8077, 3.2002, 1.6773, 3.1913]],\n",
      "\n",
      "        [[1.9844,   -inf,   -inf,   -inf,   -inf],\n",
      "         [2.3590, 3.3261,   -inf,   -inf,   -inf],\n",
      "         [1.7684, 2.6570, 3.1543,   -inf,   -inf],\n",
      "         [1.2935, 1.7119, 1.7177, 1.8196,   -inf],\n",
      "         [1.9760, 2.9873, 2.3802, 1.7826, 3.8213]]])\n",
      "Attention = tensor([[[0.5283, 0.0540, 0.7869, 0.4935, 0.4124, 0.7366, 0.9648, 0.9398],\n",
      "         [0.6305, 0.2897, 0.7210, 0.3035, 0.4377, 0.5066, 0.6944, 0.6560],\n",
      "         [0.3322, 0.5282, 0.3804, 0.6468, 0.2731, 0.5978, 0.4848, 0.6596],\n",
      "         [0.5091, 0.5280, 0.3651, 0.5103, 0.3169, 0.5097, 0.6402, 0.4861],\n",
      "         [0.5647, 0.5076, 0.4865, 0.6685, 0.2511, 0.6168, 0.6368, 0.4450]],\n",
      "\n",
      "        [[0.7252, 0.0208, 0.8473, 0.9892, 0.7676, 0.4925, 0.6577, 0.5945],\n",
      "         [0.7907, 0.3016, 0.7639, 0.7918, 0.7137, 0.7232, 0.8140, 0.5214],\n",
      "         [0.5680, 0.5882, 0.6221, 0.8821, 0.7834, 0.7552, 0.6425, 0.5493],\n",
      "         [0.4430, 0.6462, 0.7154, 0.7516, 0.8202, 0.6642, 0.6723, 0.6456],\n",
      "         [0.4261, 0.6287, 0.6571, 0.5491, 0.7202, 0.5515, 0.7763, 0.7319]]])\n"
     ]
    }
   ],
   "source": [
    "q = torch.rand((2,5,8))\n",
    "v = torch.rand((2,5,8))\n",
    "k = q\n",
    "\n",
    "K = q.shape[1]  # Sequence length\n",
    "\n",
    "dk = q.shape[1]**.5 \n",
    "I = (q@k.transpose(1,2))\n",
    "\n",
    "future_mask = torch.triu(torch.ones((K, K)), diagonal=1).bool()\n",
    "I = I.masked_fill(future_mask, float('-inf'))\n",
    "a = nn.functional.softmax(I/dk, dim=-1)@v \n",
    "print(f\"I = {I}\")\n",
    "print(f\"Attention = {a}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'future'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MASKS.get(\"future\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = nn.Dropout(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[True, True, True, True, True, True, True, True],\n",
       "         [True, True, True, True, True, True, True, True],\n",
       "         [True, True, True, True, True, True, True, True],\n",
       "         [True, True, True, True, True, True, True, True],\n",
       "         [True, True, True, True, True, True, True, True]],\n",
       "\n",
       "        [[True, True, True, True, True, True, True, True],\n",
       "         [True, True, True, True, True, True, True, True],\n",
       "         [True, True, True, True, True, True, True, True],\n",
       "         [True, True, True, True, True, True, True, True],\n",
       "         [True, True, True, True, True, True, True, True]]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d(q) == q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d65bdc5d2ce5f47a5dacc71f251d002481db9b35ba5522b1e89f66f66f443b2e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
